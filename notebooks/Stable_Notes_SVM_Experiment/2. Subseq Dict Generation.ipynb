{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30118674",
   "metadata": {},
   "source": [
    "## Subseq Dict Generation\n",
    "\n",
    "This script takes a singerwise csv file and converts it into a \"subseq dict\" (subsequence dictionary) pickle file for that singer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a6d82",
   "metadata": {},
   "source": [
    "The keys of the subseq dict file are tuples in the following format (one tuple corresponds to one stable or non-stable note):\n",
    "\n",
    "**(filename, index, label)**\n",
    "\n",
    "label is st or ns corresponding to stable or non-stable\n",
    "\n",
    "The values of the subseq dict file are dataframes having the pitch contour and gesture contours for that segment. This is just to organise the data better for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c25fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61af0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG\n",
      "Dataframe created\n",
      "AG_Aalap1_Bag.csv\n",
      "AG_Aalap1_Bahar.csv\n",
      "AG_Aalap1_Bilas.csv\n",
      "AG_Aalap1_Jaun.csv\n",
      "AG_Aalap1_Kedar.csv\n",
      "AG_Aalap1_MM.csv\n",
      "AG_Aalap1_Marwa.csv\n",
      "AG_Aalap1_Nand.csv\n",
      "AG_Aalap1_Shree.csv\n",
      "AG_Aalap2_Bag.csv\n",
      "AG_Aalap2_Bahar.csv\n",
      "AG_Aalap2_Bilas.csv\n",
      "AG_Aalap2_Jaun.csv\n",
      "AG_Aalap2_MM.csv\n",
      "AG_Aalap2_Marwa.csv\n",
      "AG_Aalap2_Nand.csv\n",
      "AG_Aalap2_Shree.csv\n",
      "AG_Pakad_Bag.csv\n",
      "AG_Pakad_Bahar.csv\n",
      "AG_Pakad_Bilas.csv\n",
      "AG_Pakad_Jaun.csv\n",
      "AG_Pakad_Kedar.csv\n",
      "AG_Pakad_MM.csv\n",
      "AG_Pakad_Marwa.csv\n",
      "AG_Pakad_Nand.csv\n",
      "AG_Pakad_Shree.csv\n",
      "\n",
      "AK\n",
      "Dataframe created\n",
      "AK_Aalap1_Bag.csv\n",
      "AK_Aalap1_Bahar.csv\n",
      "AK_Aalap1_Bilas.csv\n",
      "AK_Aalap1_Jaun.csv\n",
      "AK_Aalap1_Kedar.csv\n",
      "AK_Aalap1_MM.csv\n",
      "AK_Aalap1_Marwa.csv\n",
      "AK_Aalap1_Nand.csv\n",
      "AK_Aalap1_Shree.csv\n",
      "AK_Aalap2_Bag.csv\n",
      "AK_Aalap2_Bahar.csv\n",
      "AK_Aalap2_Bilas.csv\n",
      "AK_Aalap2_Jaun.csv\n",
      "AK_Aalap2_Kedar.csv\n",
      "AK_Aalap2_MM.csv\n",
      "AK_Aalap2_Marwa.csv\n",
      "AK_Aalap2_Nand.csv\n",
      "AK_Aalap2_Shree.csv\n",
      "AK_Pakad_Bag.csv\n",
      "AK_Pakad_Bahar.csv\n",
      "AK_Pakad_Bilas.csv\n",
      "AK_Pakad_Jaun.csv\n",
      "AK_Pakad_Kedar.csv\n",
      "AK_Pakad_MM.csv\n",
      "AK_Pakad_Marwa.csv\n",
      "AK_Pakad_Nand.csv\n",
      "AK_Pakad_Shree.csv\n",
      "\n",
      "AP\n",
      "Dataframe created\n",
      "AP_Aalap1_Bag.csv\n",
      "AP_Aalap1_Bahar.csv\n",
      "AP_Aalap1_Bilas.csv\n",
      "AP_Aalap1_Jaun.csv\n",
      "AP_Aalap1_Kedar.csv\n",
      "AP_Aalap1_MM.csv\n",
      "AP_Aalap1_Marwa.csv\n",
      "AP_Aalap1_Nand.csv\n",
      "AP_Aalap1_Shree.csv\n",
      "AP_Aalap2_Bag.csv\n",
      "AP_Aalap2_Bahar.csv\n",
      "AP_Aalap2_Bilas.csv\n",
      "AP_Aalap2_Jaun.csv\n",
      "AP_Aalap2_Kedar.csv\n",
      "AP_Aalap2_MM.csv\n",
      "AP_Aalap2_Marwa.csv\n",
      "AP_Aalap2_Nand.csv\n",
      "AP_Aalap2_Shree.csv\n",
      "AP_Pakad_Bag.csv\n",
      "AP_Pakad_Bahar.csv\n",
      "AP_Pakad_Bilas.csv\n",
      "AP_Pakad_Jaun.csv\n",
      "AP_Pakad_Kedar.csv\n",
      "AP_Pakad_MM.csv\n",
      "AP_Pakad_Marwa.csv\n",
      "AP_Pakad_Nand.csv\n",
      "AP_Pakad_Shree.csv\n",
      "\n",
      "CC\n",
      "Dataframe created\n",
      "CC_Aalap1_Bag.csv\n",
      "CC_Aalap1_Bahar.csv\n",
      "CC_Aalap1_Bilas.csv\n",
      "CC_Aalap1_Jaun.csv\n",
      "CC_Aalap1_Kedar.csv\n",
      "CC_Aalap1_MM.csv\n",
      "CC_Aalap1_Marwa.csv\n",
      "CC_Aalap1_Nand.csv\n",
      "CC_Aalap1_Shree.csv\n",
      "CC_Aalap2_Bag.csv\n",
      "CC_Aalap2_Bahar.csv\n",
      "CC_Aalap2_Bilas.csv\n",
      "CC_Aalap2_Jaun.csv\n",
      "CC_Aalap2_Kedar.csv\n",
      "CC_Aalap2_MM.csv\n",
      "CC_Aalap2_Marwa.csv\n",
      "CC_Aalap2_Nand.csv\n",
      "CC_Aalap2_Shree.csv\n",
      "CC_Pakad1_Bilas.csv\n",
      "CC_Pakad2_Bilas.csv\n",
      "CC_Pakad_Bag.csv\n",
      "CC_Pakad_Bahar.csv\n",
      "CC_Pakad_Jaun.csv\n",
      "CC_Pakad_Kedar.csv\n",
      "CC_Pakad_MM.csv\n",
      "CC_Pakad_Marwa.csv\n",
      "CC_Pakad_Nand.csv\n",
      "CC_Pakad_Shree.csv\n",
      "\n",
      "MG\n",
      "Dataframe created\n",
      "MG_Aalap1_Bag.csv\n",
      "MG_Aalap1_Bahar.csv\n",
      "MG_Aalap1_Bilas.csv\n",
      "MG_Aalap1_Jaun.csv\n",
      "MG_Aalap1_Kedar.csv\n",
      "MG_Aalap1_MM.csv\n",
      "MG_Aalap1_Marwa.csv\n",
      "MG_Aalap1_Nand.csv\n",
      "MG_Aalap1_Shree.csv\n",
      "MG_Aalap2_Bag.csv\n",
      "MG_Aalap2_Bahar.csv\n",
      "MG_Aalap2_Bilas.csv\n",
      "MG_Aalap2_Jaun.csv\n",
      "MG_Aalap2_Kedar.csv\n",
      "MG_Aalap2_MM.csv\n",
      "MG_Aalap2_Marwa.csv\n",
      "MG_Aalap2_Nand.csv\n",
      "MG_Aalap2_Shree.csv\n",
      "MG_Pakad_Bag.csv\n",
      "MG_Pakad_Bahar.csv\n",
      "MG_Pakad_Bilas.csv\n",
      "MG_Pakad_Jaun.csv\n",
      "MG_Pakad_Kedar.csv\n",
      "MG_Pakad_MM.csv\n",
      "MG_Pakad_Marwa.csv\n",
      "MG_Pakad_Nand.csv\n",
      "MG_Pakad_Shree.csv\n",
      "\n",
      "MP\n",
      "Dataframe created\n",
      "MP_Aalap1_Bag.csv\n",
      "MP_Aalap1_Bahar.csv\n",
      "MP_Aalap1_Bilas.csv\n",
      "MP_Aalap1_Jaun.csv\n",
      "MP_Aalap1_Kedar.csv\n",
      "MP_Aalap1_MM.csv\n",
      "MP_Aalap1_Marwa.csv\n",
      "MP_Aalap1_Nand.csv\n",
      "MP_Aalap1_Shree.csv\n",
      "MP_Aalap2_Bag.csv\n",
      "MP_Aalap2_Bahar.csv\n",
      "MP_Aalap2_Bilas.csv\n",
      "MP_Aalap2_Jaun.csv\n",
      "MP_Aalap2_Kedar.csv\n",
      "MP_Aalap2_MM.csv\n",
      "MP_Aalap2_Marwa.csv\n",
      "MP_Aalap2_Nand.csv\n",
      "MP_Aalap2_Shree.csv\n",
      "MP_Pakad_Bag.csv\n",
      "MP_Pakad_Bahar.csv\n",
      "MP_Pakad_Bilas.csv\n",
      "MP_Pakad_Jaun.csv\n",
      "MP_Pakad_Kedar.csv\n",
      "MP_Pakad_MM.csv\n",
      "MP_Pakad_Marwa.csv\n",
      "MP_Pakad_Nand.csv\n",
      "MP_Pakad_Shree.csv\n",
      "\n",
      "NM\n",
      "Dataframe created\n",
      "NM_Aalap1_Bag.csv\n",
      "NM_Aalap1_Bahar.csv\n",
      "NM_Aalap1_Bilas.csv\n",
      "NM_Aalap1_Jaun.csv\n",
      "NM_Aalap1_Kedar.csv\n",
      "NM_Aalap1_MM.csv\n",
      "NM_Aalap1_Marwa.csv\n",
      "NM_Aalap1_Nand.csv\n",
      "NM_Aalap1_Shree.csv\n",
      "NM_Aalap2_Bag.csv\n",
      "NM_Aalap2_Bahar.csv\n",
      "NM_Aalap2_Bilas.csv\n",
      "NM_Aalap2_Jaun.csv\n",
      "NM_Aalap2_Kedar.csv\n",
      "NM_Aalap2_MM.csv\n",
      "NM_Aalap2_Marwa.csv\n",
      "NM_Aalap2_Nand.csv\n",
      "NM_Aalap2_Shree.csv\n",
      "NM_Pakad_Bag.csv\n",
      "NM_Pakad_Bahar.csv\n",
      "NM_Pakad_Bilas.csv\n",
      "NM_Pakad_Jaun.csv\n",
      "NM_Pakad_Kedar.csv\n",
      "NM_Pakad_MM.csv\n",
      "NM_Pakad_Marwa.csv\n",
      "NM_Pakad_Nand.csv\n",
      "NM_Pakad_Shree.csv\n",
      "\n",
      "RV\n",
      "Dataframe created\n",
      "RV_Aalap1_Bag.csv\n",
      "RV_Aalap1_Bahar.csv\n",
      "RV_Aalap1_Bilas.csv\n",
      "RV_Aalap1_Jaun.csv\n",
      "RV_Aalap1_Kedar.csv\n",
      "RV_Aalap1_MM.csv\n",
      "RV_Aalap1_Marwa.csv\n",
      "RV_Aalap1_Nand.csv\n",
      "RV_Aalap1_Shree.csv\n",
      "RV_Aalap2_Bag.csv\n",
      "RV_Aalap2_Bilas.csv\n",
      "RV_Aalap2_Jaun.csv\n",
      "RV_Aalap2_Kedar.csv\n",
      "RV_Aalap2_MM.csv\n",
      "RV_Aalap2_Marwa.csv\n",
      "RV_Aalap2_Nand.csv\n",
      "RV_Aalap2_Shree.csv\n",
      "RV_Pakad_Bag.csv\n",
      "RV_Pakad_Bahar.csv\n",
      "RV_Pakad_Bilas.csv\n",
      "RV_Pakad_Jaun.csv\n",
      "RV_Pakad_Kedar.csv\n",
      "RV_Pakad_MM.csv\n",
      "RV_Pakad_Marwa.csv\n",
      "RV_Pakad_Nand.csv\n",
      "RV_Pakad_Shree.csv\n",
      "\n",
      "SCh\n",
      "Dataframe created\n",
      "SCh_Aalap1_Bag.csv\n",
      "SCh_Aalap1_Bahar.csv\n",
      "SCh_Aalap1_Bilas.csv\n",
      "SCh_Aalap1_Jaun.csv\n",
      "SCh_Aalap1_Kedar.csv\n",
      "SCh_Aalap1_MM.csv\n",
      "SCh_Aalap1_Marwa.csv\n",
      "SCh_Aalap1_Nand.csv\n",
      "SCh_Aalap1_Shree.csv\n",
      "SCh_Aalap2_Bag.csv\n",
      "SCh_Aalap2_Bahar.csv\n",
      "SCh_Aalap2_Bilas.csv\n",
      "SCh_Aalap2_Jaun.csv\n",
      "SCh_Aalap2_Kedar.csv\n",
      "SCh_Aalap2_MM.csv\n",
      "SCh_Aalap2_Marwa.csv\n",
      "SCh_Aalap2_Nand.csv\n",
      "SCh_Aalap2_Shree.csv\n",
      "SCh_Aalap3_Kedar.csv\n",
      "SCh_Aalap3_MM.csv\n",
      "SCh_Pakad1_Bag.csv\n",
      "SCh_Pakad1_Bahar.csv\n",
      "SCh_Pakad1_Bilas.csv\n",
      "SCh_Pakad1_Jaun.csv\n",
      "SCh_Pakad1_Kedar.csv\n",
      "SCh_Pakad1_MM.csv\n",
      "SCh_Pakad1_Marwa.csv\n",
      "SCh_Pakad1_Nand.csv\n",
      "SCh_Pakad1_Shree.csv\n",
      "SCh_Pakad2_Bag.csv\n",
      "SCh_Pakad2_Bahar.csv\n",
      "SCh_Pakad2_Bilas.csv\n",
      "SCh_Pakad2_Jaun.csv\n",
      "SCh_Pakad2_Kedar.csv\n",
      "SCh_Pakad2_MM.csv\n",
      "SCh_Pakad2_Marwa.csv\n",
      "SCh_Pakad2_Nand.csv\n",
      "SCh_Pakad2_Shree.csv\n",
      "\n",
      "SM\n",
      "Dataframe created\n",
      "SM_Aalap1_Bag.csv\n",
      "SM_Aalap1_Bahar.csv\n",
      "SM_Aalap1_Bilas.csv\n",
      "SM_Aalap1_Jaun.csv\n",
      "SM_Aalap1_Kedar.csv\n",
      "SM_Aalap1_MM.csv\n",
      "SM_Aalap1_Marwa.csv\n",
      "SM_Aalap1_Nand.csv\n",
      "SM_Aalap1_Shree.csv\n",
      "SM_Aalap2_Bag.csv\n",
      "SM_Aalap2_Bahar.csv\n",
      "SM_Aalap2_Bilas.csv\n",
      "SM_Aalap2_Jaun.csv\n",
      "SM_Aalap2_Kedar.csv\n",
      "SM_Aalap2_MM.csv\n",
      "SM_Aalap2_Marwa.csv\n",
      "SM_Aalap2_Nand.csv\n",
      "SM_Aalap2_Shree.csv\n",
      "SM_Pakad_Bag.csv\n",
      "SM_Pakad_Bahar.csv\n",
      "SM_Pakad_Bilas.csv\n",
      "SM_Pakad_Jaun.csv\n",
      "SM_Pakad_Kedar.csv\n",
      "SM_Pakad_MM.csv\n",
      "SM_Pakad_Marwa.csv\n",
      "SM_Pakad_Nand.csv\n",
      "SM_Pakad_Shree.csv\n",
      "\n",
      "SS\n",
      "Dataframe created\n",
      "SS_Aalap1_Bag.csv\n",
      "SS_Aalap1_Bahar.csv\n",
      "SS_Aalap1_Bilas.csv\n",
      "SS_Aalap1_Jaun.csv\n",
      "SS_Aalap1_Kedar.csv\n",
      "SS_Aalap1_MM.csv\n",
      "SS_Aalap1_Marwa.csv\n",
      "SS_Aalap1_Nand.csv\n",
      "SS_Aalap1_Shree.csv\n",
      "SS_Aalap2_Bag.csv\n",
      "SS_Aalap2_Bahar.csv\n",
      "SS_Aalap2_Bilas.csv\n",
      "SS_Aalap2_Jaun.csv\n",
      "SS_Aalap2_Kedar.csv\n",
      "SS_Aalap2_MM.csv\n",
      "SS_Aalap2_Marwa.csv\n",
      "SS_Aalap2_Nand.csv\n",
      "SS_Aalap2_Shree.csv\n",
      "SS_Pakad_Bag.csv\n",
      "SS_Pakad_Bahar.csv\n",
      "SS_Pakad_Bilas.csv\n",
      "SS_Pakad_Jaun.csv\n",
      "SS_Pakad_Kedar.csv\n",
      "SS_Pakad_MM.csv\n",
      "SS_Pakad_Marwa.csv\n",
      "SS_Pakad_Nand.csv\n",
      "SS_Pakad_Shree.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for singer in ['AG','AK','AP','CC','MG','MP','NM','RV','SCh','SM','SS']:\n",
    "    \n",
    "    print(singer)\n",
    "    df = pd.read_csv(f'./../../Data/check_output_Jan20/check_output_v2/{singer}_gesture_pitch_pd_with_vel_accln.csv')\n",
    "#     df = df.drop(columns = [x for x in df.columns.values if 'Elbow' in x])\n",
    "    ## Uncomment above line if you want to take only wrist and drop elbow contours\n",
    "    \n",
    "    filelist = list(df['filename'].unique())\n",
    "\n",
    "    notes = []\n",
    "    for filename in filelist:\n",
    "        \n",
    "        # Reading stable note and SDS timestamps\n",
    "        sds = pd.read_csv(f'./SDS_Dataframes/{filename}')\n",
    "        stable = pd.read_csv(f'./Stable_Notes/{filename}')\n",
    "        \n",
    "        # Making dicts for start and end timestamps for SDS \n",
    "        # and Stable notes separately to make a \"timestamp dict\"\n",
    "        # which will be used in marking stable and non-stable notes\n",
    "        start_times_sds = dict([(round(float(x['Start']),2),'start_sds') for _,x in sds.iterrows()])\n",
    "        end_times_sds = dict([(round(float(x['End']),2),'end_sds') for _,x in sds.iterrows()])\n",
    "        start_times_st = dict([(round(float(x['Start']),2),'start_st') for _,x in stable.iterrows()])\n",
    "        end_times_st = dict([(round(float(x['End']),2),'end_st') for _,x in stable.iterrows()])\n",
    "        timestamp_dict = start_times_sds | end_times_sds | start_times_st | end_times_st\n",
    "        \n",
    "        myKeys = list(timestamp_dict.keys())\n",
    "        myKeys.sort()\n",
    "        sorted_dict = {i: timestamp_dict[i] for i in myKeys}\n",
    "\n",
    "        st = None\n",
    "        ns = None\n",
    "        \n",
    "        # Iterate over each timestamp to mark stable and non-stable notes\n",
    "        for timestamp in list(sorted_dict.keys()):\n",
    "            if sorted_dict[timestamp] == 'start_sds':\n",
    "                ns = timestamp\n",
    "            elif sorted_dict[timestamp] == 'start_st':\n",
    "                st = timestamp\n",
    "                notes.append([ns,timestamp,'ns',filename])\n",
    "            elif sorted_dict[timestamp] == 'end_st':\n",
    "                notes.append([st,timestamp,'st',filename])\n",
    "            elif sorted_dict[timestamp] == 'end_sds':\n",
    "                notes.append([ns,timestamp,'ns',filename])\n",
    "\n",
    "    notes = pd.DataFrame(notes,columns=['Start','End','Target','Filename'])\n",
    "    print('Dataframe created')\n",
    "    \n",
    "    notes['Duration'] = notes['End']-notes['Start']\n",
    "    # Drop notes which are not in the range [0.5,5]s\n",
    "    notes = notes.loc[notes['Duration'].apply(lambda x: x >= 0.5 and x < 5)].reset_index(drop=True)\n",
    "\n",
    "    subseq_dict_new = {}\n",
    "    \n",
    "    # Gesture processing starts here\n",
    "    for filename in filelist:\n",
    "\n",
    "        notes_this_file = notes.loc[notes['Filename']==filename].reset_index(drop=True)\n",
    "        data_this_file = df.loc[df['filename']==filename].reset_index(drop=True)\n",
    "        \n",
    "        for i in notes_this_file.index.values:\n",
    "            # Cut out dataframe for this note from the overall data for this alap\n",
    "            note_gesture = data_this_file.loc[data_this_file['time'].apply(lambda x: x >= notes_this_file['Start'].iloc[i]\\\n",
    "                                                             and x <= notes_this_file['End'].iloc[i])]\n",
    "            # Make an entry for this note in our \"subseq dict\"\n",
    "            subseq_dict_new[(filename, i, notes_this_file['Target'].iloc[i])] = note_gesture\n",
    "        print(filename)\n",
    "        \n",
    "    # Save the subseq dict file for further tasks\n",
    "    with open(f'./Subseq_Dict_With_Elbow/subseq_dict_{singer}.pkl', 'wb') as file:\n",
    "        pickle.dump(subseq_dict_new, file)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58f07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
